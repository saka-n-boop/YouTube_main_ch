import json
import os
import re
import sys
from datetime import datetime, timedelta
import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build

SCOPES = ['https://www.googleapis.com/auth/spreadsheets']

def get_env_vars():
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã€èªè¨¼æƒ…å ±ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’å–å¾—"""
    api_key = os.environ.get("YOUTUBE_API_KEY")
    spreadsheet_id = os.environ.get("SPREADSHEET_ID")
    service_account_key = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")

    if not api_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'YOUTUBE_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not spreadsheet_id:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'SPREADSHEET_ID' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not service_account_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'GCP_SERVICE_ACCOUNT_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    return api_key, spreadsheet_id, service_account_key

def read_channel_ids(file_path):
    """channel_ID.txt ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«IDã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(file_path):
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(1)
        
    with open(file_path, 'r', encoding='utf-8') as file:
        # ç©ºè¡Œã‚’é™¤å»ã—ã¦ãƒªã‚¹ãƒˆåŒ–
        channel_ids = [line.strip() for line in file if line.strip()]
    
    if not channel_ids:
        print("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ãƒãƒ«IDãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
        
    return channel_ids

def jst_to_utc(jst_str):
    """JSTæ—¥æ™‚æ–‡å­—åˆ—ã‚’UTCã®ISO8601ã«å¤‰æ›"""
    jst_dt = datetime.strptime(jst_str, "%Y-%m-%d %H:%M:%S")
    utc_dt = jst_dt - timedelta(hours=9)
    return utc_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

def iso8601_to_duration(iso_duration):
    """PTè¡¨è¨˜ï¼ˆYouTube ISO8601ï¼‰ã‚’HH:MM:SSåŒ–"""
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(iso_duration)
    if not match:
        return "00:00:00"
    hours = int(match.group(1)) if match.group(1) else 0
    minutes = int(match.group(2)) if match.group(2) else 0
    seconds = int(match.group(3)) if match.group(3) else 0
    return str(timedelta(hours=hours, minutes=minutes, seconds=seconds))

def convert_to_japan_time(utc_time):
    """UTCæ™‚åˆ»ã‚’JSTå¤‰æ›ã—è¡¨ç¤ºç”¨ã«"""
    utc_datetime = datetime.strptime(utc_time, "%Y-%m-%dT%H:%M:%SZ")
    japan_datetime = utc_datetime + timedelta(hours=9)
    return japan_datetime.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_time():
    """ç¾åœ¨æ™‚åˆ» (JSTè¡¨ç¤º)"""
    now_utc = datetime.utcnow()
    now_jst = now_utc + timedelta(hours=9)
    return now_jst.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_digit_date():
    """ä»Šæ—¥ã®æ—¥ä»˜ (JST, ã‚·ãƒ¼ãƒˆåç”¨ 'YYYYMMDD' ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)"""
    now_utc = datetime.utcnow()
    now_jst = now_utc + timedelta(hours=9)
    return now_jst.strftime("%Y%m%d")

def calc_engagement_rate(like_count, comment_count, view_count):
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ (ï¼…)"""
    if view_count == 0:
        return 0.0
    return round((like_count + comment_count) / view_count * 100, 2)

def get_youtube_data_by_channel(api_key, channel_id, start_datetime_jst, end_datetime_jst, max_total_results=500):
    """
    æŒ‡å®šãƒãƒ£ãƒ³ãƒãƒ«ãƒ»æœŸé–“ã®å‹•ç”»æƒ…å ±ã‚’å–å¾—
    â€» search APIã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€APIã‚³ã‚¹ãƒˆ(100/req)ã«æ³¨æ„ã€‚
    â€» max_total_resultsã¯å¤šã‚ã«è¨­å®šã—ã¦ã„ã¾ã™ãŒã€APIåˆ¶é™è€ƒæ…®ã®ãŸã‚èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
    """
    youtube = build('youtube', 'v3', developerKey=api_key)
    start_utc = jst_to_utc(start_datetime_jst)
    end_utc = jst_to_utc(end_datetime_jst)
    start_dt = datetime.strptime(start_datetime_jst, "%Y-%m-%d %H:%M:%S")
    end_dt = datetime.strptime(end_datetime_jst, "%Y-%m-%d %H:%M:%S")

    video_ids = []
    next_page_token = None

    # æ¤œç´¢ãƒ«ãƒ¼ãƒ—ï¼ˆæŒ‡å®šæœŸé–“å†…ã®å‹•ç”»IDã‚’åé›†ï¼‰
    while len(video_ids) < max_total_results:
        try:
            search_response = youtube.search().list(
                channelId=channel_id,  # ãƒãƒ£ãƒ³ãƒãƒ«IDæŒ‡å®š
                part='snippet',
                type='video',          # å‹•ç”»ã®ã¿
                order='date',          # æ—¥ä»˜é †ï¼ˆæ–°ã—ã„é †ï¼‰
                maxResults=min(50, max_total_results - len(video_ids)),
                publishedAfter=start_utc,
                publishedBefore=end_utc,
                pageToken=next_page_token
            ).execute()
        except Exception as e:
            print(f"   âš ï¸ APIã‚¨ãƒ©ãƒ¼ (Channel ID: {channel_id}): {e}")
            break

        video_ids += [item['id']['videoId'] for item in search_response['items']]
        next_page_token = search_response.get('nextPageToken')
        
        # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒãªã„ã€ã¾ãŸã¯ä¸Šé™ã«é”ã—ãŸã‚‰çµ‚äº†
        if not next_page_token or len(video_ids) >= max_total_results:
            break

    # è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆçµ±è¨ˆæƒ…å ±ãªã©ï¼‰
    video_data = []
    # 50ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†
    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        try:
            video_response = youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=','.join(batch_ids)
            ).execute()

            for item in video_response['items']:
                snippet = item['snippet']
                statistics = item.get('statistics', {})
                content_details = item['contentDetails']

                published_at_utc = snippet['publishedAt']
                published_at_jst = datetime.strptime(published_at_utc, "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)

                # å¿µã®ãŸã‚ã®æœŸé–“ãƒã‚§ãƒƒã‚¯
                if not (start_dt <= published_at_jst <= end_dt):
                    continue

                video_data.append({
                    'title': snippet['title'],
                    'channel': snippet['channelTitle'],
                    'published_at': snippet['publishedAt'],
                    'video_id': item['id'],
                    'view_count': int(statistics.get('viewCount', 0)),
                    'like_count': int(statistics.get('likeCount', 0)),
                    'comment_count': int(statistics.get('commentCount', 0)),
                    'duration': content_details.get('duration', "PT0S")
                })
        except Exception as e:
            print(f"   âš ï¸ è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return video_data

def merge_and_deduplicate(video_data_list):
    """
    è¤‡æ•°ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒªã‚¹ãƒˆã‚’çµ±åˆã—ã€é‡è¤‡ã‚’æ’é™¤ï¼ˆvideo_idåŸºæº–ï¼‰
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯è¡Œã‚ãšã€å–å¾—ã—ãŸå…¨å‹•ç”»ã‚’å¯¾è±¡ã¨ã™ã‚‹
    """
    merged = {}
    for video_data in video_data_list:
        for video in video_data:
            # video_idã‚’ã‚­ãƒ¼ã«ã—ã¦ä¸Šæ›¸ãï¼ˆé‡è¤‡æ’é™¤ï¼‰
            merged[video['video_id']] = video
    
    # è¾æ›¸ã®å€¤ï¼ˆå‹•ç”»ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ãƒªã‚¹ãƒˆã«æˆ»ã—ã¦è¿”å´
    return list(merged.values())

def export_to_google_sheet(video_data, spreadsheet_id, service_account_key, exec_time_jst, sheet_name):
    """
    Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
    """
    # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼
    credentials_dict = json.loads(service_account_key)
    creds = Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(spreadsheet_id)

    # æ–°ã—ã„ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ
    try:
        worksheet = sh.add_worksheet(title=sheet_name, rows=str(len(video_data)+10), cols="20")
    except gspread.exceptions.APIError as e:
        # ã‚·ãƒ¼ãƒˆãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆãªã©ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        print(f"âš ï¸ ã‚·ãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼ï¼ˆæ—¢ã«å­˜åœ¨ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰: {e}")
        worksheet = sh.worksheet(sheet_name)
        worksheet.clear() # æ—¢å­˜ã®å ´åˆã¯ã‚¯ãƒªã‚¢ã—ã¦ä¸Šæ›¸ã

    headers = [
        "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«", "ãƒãƒ£ãƒ³ãƒãƒ«å", "æŠ•ç¨¿æ—¥æ™‚ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰", "å‹•ç”»ID",
        "å‹•ç”»URL", "å†ç”Ÿå›æ•°", "é«˜è©•ä¾¡æ•°", "è¦–è´è€…ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å‹•ç”»ã®é•·ã•",
        "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡(%)", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œæ™‚é–“ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰"
    ]
    rows = []
    for video in video_data:
        engagement_rate = calc_engagement_rate(video['like_count'], video['comment_count'], video['view_count'])
        video_url = f"https://www.youtube.com/watch?v={video['video_id']}"
        rows.append([
            video['title'],
            video['channel'],
            convert_to_japan_time(video['published_at']),
            video['video_id'],
            video_url,
            video['view_count'],
            video['like_count'],
            video['comment_count'],
            iso8601_to_duration(video['duration']),
            engagement_rate,
            exec_time_jst
        ])
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
    worksheet.clear()
    worksheet.append_row(headers)
    if rows:
        worksheet.append_rows(rows, value_input_option='USER_ENTERED')

def main():
    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    channel_id_file = 'channel_ID.txt'

    # ç’°å¢ƒå¤‰æ•°ã¨è¨­å®šã®èª­ã¿è¾¼ã¿
    api_key, spreadsheet_id, service_account_key = get_env_vars()
    channel_ids = read_channel_ids(channel_id_file)

    # æ—¥ä»˜è¨­å®š
    sheet_name = get_current_japan_digit_date()
    exec_time_jst = get_current_japan_time()
    
    # æ¤œç´¢æœŸé–“è¨­å®šï¼ˆ2020å¹´1æœˆ1æ—¥ ã€œ ä»Šæ—¥ã®10:01:00ï¼‰
    # â€»æ¯æ—¥å®Ÿè¡Œã—ã¦ã‚‚ã€Œ2020å¹´ã‹ã‚‰ã®å…¨ãƒªã‚¹ãƒˆã€ã‚’å–å¾—ã™ã‚‹ä»•æ§˜ã§ã™
    start_datetime_jst = "2022-01-01 00:00:00"
    end_datetime_jst = f"{sheet_name[:4]}-{sheet_name[4:6]}-{sheet_name[6:]} 23:59:59"

    # --- ã‚·ãƒ¼ãƒˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆAPIã‚¢ã‚¯ã‚»ã‚¹å‰ï¼‰ ---
    try:
        credentials_dict = json.loads(service_account_key)
        creds = Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
        gc = gspread.authorize(creds)
        sh = gc.open_by_key(spreadsheet_id)
        existing_sheets = [ws.title for ws in sh.worksheets()]
        
        if sheet_name in existing_sheets:
            print(f"âœ… {sheet_name}ã‚·ãƒ¼ãƒˆã¯æ—¢ã«å­˜åœ¨ã—ã¦ã„ã‚‹ãŸã‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚IDã‚„æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n{e}")
        sys.exit(1)

    # --- YouTube Data APIã‚¢ã‚¯ã‚»ã‚¹ ---
    video_data_list = []
    print(f"â¡ï¸ YouTubeãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«: {len(channel_ids)}ä»¶, æœŸé–“: {start_datetime_jst} ã€œ)")
    
    for channel_id in channel_ids:
        print(f"   - ãƒãƒ£ãƒ³ãƒãƒ«ID '{channel_id}' æ¤œç´¢ä¸­...")
        # å„ãƒãƒ£ãƒ³ãƒãƒ«æœ€å¤§500ä»¶ã¾ã§å–å¾—ï¼ˆAPIã‚³ã‚¹ãƒˆç¯€ç´„ã®ãŸã‚åˆ¶é™ã‚’è¨­ã‘ã¦ã„ã¾ã™ï¼‰
        video_data = get_youtube_data_by_channel(
            api_key, 
            channel_id, 
            start_datetime_jst, 
            end_datetime_jst, 
            max_total_results=500
        )
        video_data_list.append(video_data)
        print(f"     -> {len(video_data)}ä»¶å–å¾—")

    # ãƒ‡ãƒ¼ã‚¿çµ±åˆã€é‡è¤‡æ’é™¤ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
    merged_video_data = merge_and_deduplicate(video_data_list)
    print(f"â¡ï¸ é‡è¤‡æ’é™¤å¾Œã®ç·å‹•ç”»æ•°: {len(merged_video_data)}ä»¶")
    
    if not merged_video_data:
        print("âš ï¸ å¯¾è±¡æœŸé–“ã®å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # å†ç”Ÿå›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    merged_video_data.sort(key=lambda x: x['view_count'], reverse=True)
    
    # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
    print("â¡ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸å‡ºåŠ›ä¸­...")
    export_to_google_sheet(merged_video_data, spreadsheet_id, service_account_key, exec_time_jst, sheet_name)
    print(f"ğŸ‰ å‡¦ç†å®Œäº†ï¼ˆã‚·ãƒ¼ãƒˆå: {sheet_name}ï¼‰")

if __name__ == "__main__":
    main()


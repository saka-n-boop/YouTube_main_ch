import json
import os
import re
import sys
from datetime import datetime, timedelta, timezone  # timezoneã‚’è¿½åŠ 
import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from dateutil import parser as date_parser

SCOPES = ['https://www.googleapis.com/auth/spreadsheets']

def get_env_vars():
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã€èªè¨¼æƒ…å ±ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’å–å¾—"""
    api_key = os.environ.get("YOUTUBE_API_KEY")
    spreadsheet_id = os.environ.get("SPREADSHEET_ID")
    service_account_key = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")

    if not api_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'YOUTUBE_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not spreadsheet_id:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'SPREADSHEET_ID' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not service_account_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'GCP_SERVICE_ACCOUNT_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    return api_key, spreadsheet_id, service_account_key

def read_channel_ids(file_path):
    """channel_ID.txt ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«IDã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡æ’é™¤ï¼‰"""
    if not os.path.exists(file_path):
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(1)
        
    with open(file_path, 'r', encoding='utf-8') as file:
        channel_ids = [line.strip() for line in file if line.strip()]
    
    unique_ids = list(set(channel_ids))
    
    if not unique_ids:
        print("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ãƒãƒ«IDãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
        
    return unique_ids

def iso8601_to_duration(iso_duration):
    """PTè¡¨è¨˜ï¼ˆYouTube ISO8601ï¼‰ã‚’HH:MM:SSåŒ–"""
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(iso_duration)
    if not match:
        return "00:00:00"
    hours = int(match.group(1)) if match.group(1) else 0
    minutes = int(match.group(2)) if match.group(2) else 0
    seconds = int(match.group(3)) if match.group(3) else 0
    return str(timedelta(hours=hours, minutes=minutes, seconds=seconds))

def convert_to_japan_time(utc_time_str):
    """UTCæ™‚åˆ»æ–‡å­—åˆ—ã‚’JSTå¤‰æ›ã—è¡¨ç¤ºç”¨ã«"""
    # dateutilã§ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ä»˜ãã«ãªã‚‹ï¼‰
    utc_dt = date_parser.parse(utc_time_str)
    
    # JSTã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    JST = timezone(timedelta(hours=9))
    
    # JSTã«å¤‰æ›
    japan_dt = utc_dt.astimezone(JST)
    
    return japan_dt.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_time():
    """ç¾åœ¨æ™‚åˆ» (JSTè¡¨ç¤º)"""
    # UTCç¾åœ¨æ™‚åˆ»ã‚’å–å¾—ã—ã€JSTã«å¤‰æ›
    now_utc = datetime.now(timezone.utc)
    JST = timezone(timedelta(hours=9))
    now_jst = now_utc.astimezone(JST)
    return now_jst.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_digit_date():
    """ä»Šæ—¥ã®æ—¥ä»˜ (JST, ã‚·ãƒ¼ãƒˆåç”¨ 'YYYYMMDD' ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)"""
    now_utc = datetime.now(timezone.utc)
    JST = timezone(timedelta(hours=9))
    now_jst = now_utc.astimezone(JST)
    return now_jst.strftime("%Y%m%d")

def calc_engagement_rate(like_count, comment_count, view_count):
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ (ï¼…)"""
    if view_count == 0:
        return 0.0
    return round((like_count + comment_count) / view_count * 100, 2)

def get_uploads_playlist_id(youtube, channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«IDã‹ã‚‰ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆIDã€ã‚’å–å¾—"""
    try:
        response = youtube.channels().list(
            id=channel_id,
            part='contentDetails'
        ).execute()
        
        if not response['items']:
            return None
        
        return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
    except Exception as e:
        print(f"   âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({channel_id}): {e}")
        return None

def get_all_videos_since_2020(api_key, channel_id):
    """
    æŒ‡å®šãƒãƒ£ãƒ³ãƒãƒ«ã®2020å¹´ä»¥é™ã®å…¨å‹•ç”»ã‚’å–å¾—ï¼ˆPlaylistItemsä½¿ç”¨ï¼‰
    """
    youtube = build('youtube', 'v3', developerKey=api_key)
    
    # 1. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆIDã‚’å–å¾—
    uploads_playlist_id = get_uploads_playlist_id(youtube, channel_id)
    if not uploads_playlist_id:
        print(f"   âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã¾ãŸã¯å–å¾—ã§ãã¾ã›ã‚“: {channel_id}")
        return []

    # 2020å¹´1æœˆ1æ—¥ (UTC, aware) ã‚’å¢ƒç•Œç·šã¨ã™ã‚‹
    cutoff_date = datetime(2020, 1, 1, 0, 0, 0, tzinfo=timezone.utc)

    video_ids = []
    next_page_token = None
    is_fetching = True

    # 2. ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‹ã‚‰å‹•ç”»IDãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ–°ã—ã„é †ã«å–å¾—ã•ã‚Œã‚‹ï¼‰
    while is_fetching:
        try:
            pl_request = youtube.playlistItems().list(
                playlistId=uploads_playlist_id,
                part='snippet',
                maxResults=50,
                pageToken=next_page_token
            )
            pl_response = pl_request.execute()

            for item in pl_response['items']:
                published_at_str = item['snippet']['publishedAt']
                dt = date_parser.parse(published_at_str)

                # æ™‚åˆ»æ¯”è¼ƒ
                if dt < cutoff_date:
                    is_fetching = False
                    break
                
                video_ids.append(item['snippet']['resourceId']['videoId'])

            next_page_token = pl_response.get('nextPageToken')
            if not next_page_token:
                break
        except Exception as e:
            print(f"   âš ï¸ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            break

    # 3. è©³ç´°æƒ…å ±ã‚’å–å¾—
    final_video_data = []
    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        try:
            vid_response = youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=','.join(batch_ids)
            ).execute()

            for item in vid_response['items']:
                snippet = item['snippet']
                statistics = item.get('statistics', {})
                content_details = item['contentDetails']

                final_video_data.append({
                    'title': snippet['title'],
                    'channel': snippet['channelTitle'],
                    'published_at': snippet['publishedAt'],
                    'video_id': item['id'],
                    'view_count': int(statistics.get('viewCount', 0)),
                    'like_count': int(statistics.get('likeCount', 0)),
                    'comment_count': int(statistics.get('commentCount', 0)),
                    'duration': content_details.get('duration', "PT0S")
                })
        except Exception as e:
            print(f"   âš ï¸ è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return final_video_data

def export_to_google_sheet(video_data, spreadsheet_id, service_account_key, exec_time_jst, sheet_name):
    """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›"""
    credentials_dict = json.loads(service_account_key)
    creds = Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(spreadsheet_id)

    # ã‚·ãƒ¼ãƒˆä½œæˆã¾ãŸã¯å–å¾—
    try:
        worksheet = sh.add_worksheet(title=sheet_name, rows=str(len(video_data)+100), cols="20")
    except gspread.exceptions.APIError:
        worksheet = sh.worksheet(sheet_name)
        worksheet.clear()

    headers = [
        "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«", "ãƒãƒ£ãƒ³ãƒãƒ«å", "æŠ•ç¨¿æ—¥æ™‚ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰", "å‹•ç”»ID",
        "å‹•ç”»URL", "å†ç”Ÿå›æ•°", "é«˜è©•ä¾¡æ•°", "è¦–è´è€…ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å‹•ç”»ã®é•·ã•",
        "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡(%)", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œæ™‚é–“ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰"
    ]
    rows = []
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
    for video in video_data:
        engagement_rate = calc_engagement_rate(video['like_count'], video['comment_count'], video['view_count'])
        video_url = f"https://www.youtube.com/watch?v={video['video_id']}"
        
        # ã“ã“ã§ convert_to_japan_time ã‚’å‘¼ã¶ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰
        jst_time = convert_to_japan_time(video['published_at'])
        
        rows.append([
            video['title'],
            video['channel'],
            jst_time,
            video['video_id'],
            video_url,
            video['view_count'],
            video['like_count'],
            video['comment_count'],
            iso8601_to_duration(video['duration']),
            engagement_rate,
            exec_time_jst
        ])
    
    # æ›¸ãè¾¼ã¿å®Ÿè¡Œï¼ˆãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„ã®ã§ãƒãƒƒãƒæ›´æ–°ã‚’ä½¿ç”¨ï¼‰
    worksheet.update('A1', [headers])
    if rows:
        # ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„å ´åˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ã¨ã—ã¦ã€æœ¬æ¥ã¯åˆ†å‰²ã™ã¹ãã§ã™ãŒ
        # gspreadã®updateã¯æ¯”è¼ƒçš„é«˜é€Ÿãªã®ã§ã¾ãšã¯ä¸€æ‹¬ã§è©¦ã¿ã¾ã™
        worksheet.update('A2', rows, value_input_option='USER_ENTERED')

def main():
    channel_id_file = 'channel_ID.txt'
    api_key, spreadsheet_id, service_account_key = get_env_vars()
    
    channel_ids = read_channel_ids(channel_id_file)
    sheet_name = get_current_japan_digit_date()
    exec_time_jst = get_current_japan_time()

    print(f"â¡ï¸ YouTubeãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«: {len(channel_ids)}ä»¶, 2020å¹´ä»¥é™)")

    all_video_data = []

    for idx, channel_id in enumerate(channel_ids, 1):
        print(f"   [{idx}/{len(channel_ids)}] Channel ID: {channel_id} å‡¦ç†ä¸­...")
        channel_videos = get_all_videos_since_2020(api_key, channel_id)
        print(f"     -> {len(channel_videos)}ä»¶ å–å¾—å®Œäº†")
        all_video_data.extend(channel_videos)

    if not all_video_data:
        print("âš ï¸ å‹•ç”»ãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # å…¨ä½“ã®é‡è¤‡æ’é™¤
    unique_videos = {v['video_id']: v for v in all_video_data}.values()
    final_list = list(unique_videos)

    # å†ç”Ÿå›æ•°é †ã«ã‚½ãƒ¼ãƒˆ
    final_list.sort(key=lambda x: x['view_count'], reverse=True)

    print(f"â¡ï¸ åˆè¨ˆ {len(final_list)} ä»¶ã®å‹•ç”»ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ã—ã¾ã™...")
    export_to_google_sheet(final_list, spreadsheet_id, service_account_key, exec_time_jst, sheet_name)
    print(f"ğŸ‰ å‡¦ç†å®Œäº†ï¼ˆã‚·ãƒ¼ãƒˆå: {sheet_name}ï¼‰")

if __name__ == "__main__":
    main()

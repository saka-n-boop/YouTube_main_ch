import json
import os
import re
import sys
from datetime import datetime, timedelta, timezone
import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from dateutil import parser as date_parser

SCOPES = ['https://www.googleapis.com/auth/spreadsheets']

def get_env_vars():
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã€èªè¨¼æƒ…å ±ã€2ã¤ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’å–å¾—"""
    api_key = os.environ.get("YOUTUBE_API_KEY")
    spreadsheet_id = os.environ.get("SPREADSHEET_ID")         # å±¥æ­´ä¿å­˜ç”¨
    dist_spreadsheet_id = os.environ.get("DIST_SPREADSHEET_ID") # é…å¸ƒç”¨
    service_account_key = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")

    if not api_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'YOUTUBE_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not spreadsheet_id:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'SPREADSHEET_ID' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not dist_spreadsheet_id:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'DIST_SPREADSHEET_ID' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if not service_account_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'GCP_SERVICE_ACCOUNT_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    return api_key, spreadsheet_id, dist_spreadsheet_id, service_account_key

def read_channel_ids(file_path):
    if not os.path.exists(file_path):
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(1)
    with open(file_path, 'r', encoding='utf-8') as file:
        channel_ids = [line.strip() for line in file if line.strip()]
    unique_ids = list(set(channel_ids))
    if not unique_ids:
        print("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ãƒãƒ«IDãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    return unique_ids

def iso8601_to_duration(iso_duration):
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(iso_duration)
    if not match:
        return "00:00:00"
    hours = int(match.group(1)) if match.group(1) else 0
    minutes = int(match.group(2)) if match.group(2) else 0
    seconds = int(match.group(3)) if match.group(3) else 0
    return str(timedelta(hours=hours, minutes=minutes, seconds=seconds))

def convert_to_japan_time(utc_time_str):
    utc_dt = date_parser.parse(utc_time_str)
    JST = timezone(timedelta(hours=9))
    japan_dt = utc_dt.astimezone(JST)
    return japan_dt.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_time():
    now_utc = datetime.now(timezone.utc)
    JST = timezone(timedelta(hours=9))
    now_jst = now_utc.astimezone(JST)
    return now_jst.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_digit_date():
    now_utc = datetime.now(timezone.utc)
    JST = timezone(timedelta(hours=9))
    now_jst = now_utc.astimezone(JST)
    return now_jst.strftime("%Y%m%d")

def calc_engagement_rate(like_count, comment_count, view_count):
    if view_count == 0:
        return 0.0
    return round((like_count + comment_count) / view_count * 100, 2)

def get_uploads_playlist_id(youtube, channel_id):
    try:
        response = youtube.channels().list(
            id=channel_id,
            part='contentDetails'
        ).execute()
        if not response['items']:
            return None
        return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
    except Exception as e:
        print(f"   âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({channel_id}): {e}")
        return None

def get_all_videos_since_2020(api_key, channel_id):
    youtube = build('youtube', 'v3', developerKey=api_key)
    uploads_playlist_id = get_uploads_playlist_id(youtube, channel_id)
    if not uploads_playlist_id:
        print(f"   âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã¾ãŸã¯å–å¾—ã§ãã¾ã›ã‚“: {channel_id}")
        return []

    cutoff_date = datetime(2020, 1, 1, 0, 0, 0, tzinfo=timezone.utc)
    video_ids = []
    next_page_token = None
    is_fetching = True

    while is_fetching:
        try:
            pl_request = youtube.playlistItems().list(
                playlistId=uploads_playlist_id,
                part='snippet',
                maxResults=50,
                pageToken=next_page_token
            )
            pl_response = pl_request.execute()
            for item in pl_response['items']:
                published_at_str = item['snippet']['publishedAt']
                dt = date_parser.parse(published_at_str)
                if dt < cutoff_date:
                    is_fetching = False
                    break
                video_ids.append(item['snippet']['resourceId']['videoId'])
            next_page_token = pl_response.get('nextPageToken')
            if not next_page_token:
                break
        except Exception as e:
            print(f"   âš ï¸ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            break

    final_video_data = []
    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        try:
            vid_response = youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=','.join(batch_ids)
            ).execute()
            for item in vid_response['items']:
                snippet = item['snippet']
                statistics = item.get('statistics', {})
                content_details = item['contentDetails']
                final_video_data.append({
                    'title': snippet['title'],
                    'channel': snippet['channelTitle'],
                    'published_at': snippet['publishedAt'],
                    'video_id': item['id'],
                    'view_count': int(statistics.get('viewCount', 0)),
                    'like_count': int(statistics.get('likeCount', 0)),
                    'comment_count': int(statistics.get('commentCount', 0)),
                    'duration': content_details.get('duration', "PT0S")
                })
        except Exception as e:
            print(f"   âš ï¸ è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    return final_video_data

def prepare_rows(video_data, exec_time_jst):
    headers = [
        "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«", "ãƒãƒ£ãƒ³ãƒãƒ«å", "æŠ•ç¨¿æ—¥æ™‚ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰", "å‹•ç”»ID",
        "å‹•ç”»URL", "å†ç”Ÿå›æ•°", "é«˜è©•ä¾¡æ•°", "è¦–è´è€…ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å‹•ç”»ã®é•·ã•",
        "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡(%)", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œæ™‚é–“ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰"
    ]
    rows = []
    for video in video_data:
        engagement_rate = calc_engagement_rate(video['like_count'], video['comment_count'], video['view_count'])
        video_url = f"https://www.youtube.com/watch?v={video['video_id']}"
        jst_time = convert_to_japan_time(video['published_at'])
        rows.append([
            video['title'],
            video['channel'],
            jst_time,
            video['video_id'],
            video_url,
            video['view_count'],
            video['like_count'],
            video['comment_count'],
            iso8601_to_duration(video['duration']),
            engagement_rate,
            exec_time_jst
        ])
    return headers, rows

def save_to_history_sheet(gc, spreadsheet_id, sheet_name, headers, rows):
    """ã€å±¥æ­´ç”¨ã€‘æ–°è¦ã‚·ãƒ¼ãƒˆä½œæˆï¼ˆåŒåã‚·ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å‰æï¼‰"""
    sh = gc.open_by_key(spreadsheet_id)
    # ã“ã“ã§ã¯å˜ç´”ã«add_worksheetã™ã‚‹ã ã‘ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ã¯mainé–¢æ•°ã§æ¸ˆã¿ï¼‰
    worksheet = sh.add_worksheet(title=sheet_name, rows=str(len(rows)+100), cols="20")
    
    worksheet.update('A1', [headers])
    if rows:
        worksheet.update('A2', rows, value_input_option='USER_ENTERED')
    print(f"âœ… å±¥æ­´ç”¨ã‚·ãƒ¼ãƒˆ({sheet_name})ã«ä¿å­˜å®Œäº†")

def save_to_distribution_sheet(gc, dist_spreadsheet_id, headers, rows):
    """ã€é…å¸ƒç”¨ã€‘ä¸Šæ›¸ã"""
    sh = gc.open_by_key(dist_spreadsheet_id)
    worksheet = sh.get_worksheet(0)
    worksheet.clear()
    worksheet.update_title("Latest_Data")
    worksheet.update('A1', [headers])
    if rows:
        worksheet.update('A2', rows, value_input_option='USER_ENTERED')
    print(f"âœ… é…å¸ƒç”¨ã‚·ãƒ¼ãƒˆ(Latest_Data)ã‚’ä¸Šæ›¸ãæ›´æ–°å®Œäº†")

def check_if_processed(service_account_key, spreadsheet_id, sheet_name):
    """å±¥æ­´ç”¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ—¢ã«ä»Šæ—¥ã®ã‚·ãƒ¼ãƒˆãŒã‚ã‚‹ã‹ç¢ºèª"""
    try:
        credentials_dict = json.loads(service_account_key)
        creds = Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
        gc = gspread.authorize(creds)
        sh = gc.open_by_key(spreadsheet_id)
        
        existing_sheets = [ws.title for ws in sh.worksheets()]
        if sheet_name in existing_sheets:
            return True, gc # å­˜åœ¨ã™ã‚‹ã®ã§Trueã¨ã€ã¤ã„ã§ã«èªè¨¼æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è¿”ã™
        return False, gc
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

def main():
    channel_id_file = 'channel_ID.txt'
    api_key, spreadsheet_id, dist_spreadsheet_id, service_account_key = get_env_vars()
    
    sheet_name = get_current_japan_digit_date()
    
    # --- ã€ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½ã€‘å®Ÿè¡Œæ¸ˆã¿ãƒã‚§ãƒƒã‚¯ ---
    is_processed, gc = check_if_processed(service_account_key, spreadsheet_id, sheet_name)
    if is_processed:
        print(f"âœ… ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã¯æ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã€æœ¬æ—¥ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        print("   (é…å¸ƒç”¨ã‚·ãƒ¼ãƒˆã®æ›´æ–°ã‚‚è¡Œã„ã¾ã›ã‚“)")
        return
    # ------------------------------------

    channel_ids = read_channel_ids(channel_id_file)
    exec_time_jst = get_current_japan_time()

    print(f"â¡ï¸ YouTubeãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«: {len(channel_ids)}ä»¶, 2020å¹´ä»¥é™)")
    all_video_data = []
    for idx, channel_id in enumerate(channel_ids, 1):
        print(f"   [{idx}/{len(channel_ids)}] Channel ID: {channel_id} å‡¦ç†ä¸­...")
        channel_videos = get_all_videos_since_2020(api_key, channel_id)
        print(f"     -> {len(channel_videos)}ä»¶ å–å¾—å®Œäº†")
        all_video_data.extend(channel_videos)

    if not all_video_data:
        print("âš ï¸ å‹•ç”»ãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    unique_videos = {v['video_id']: v for v in all_video_data}.values()
    final_list = list(unique_videos)
    final_list.sort(key=lambda x: x['view_count'], reverse=True)

    print(f"â¡ï¸ åˆè¨ˆ {len(final_list)} ä»¶ã®å‹•ç”»ã‚’å‡ºåŠ›ã—ã¾ã™...")

    headers, rows = prepare_rows(final_list, exec_time_jst)

    # ãƒã‚§ãƒƒã‚¯æ™‚ã«ä½œã£ãŸgc(gspread client)ã‚’å†åˆ©ç”¨ã—ã¦æ›¸ãè¾¼ã¿
    # 1. å±¥æ­´ç”¨ã¸ä¿å­˜
    save_to_history_sheet(gc, spreadsheet_id, sheet_name, headers, rows)

    # 2. é…å¸ƒç”¨ã¸ä¸Šæ›¸ãä¿å­˜
    save_to_distribution_sheet(gc, dist_spreadsheet_id, headers, rows)

    print(f"ğŸ‰ å…¨å‡¦ç†å®Œäº†")

if __name__ == "__main__":
    main()
